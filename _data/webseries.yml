- title: "Video denoising in starlight using a learned, physics-informed noise model"
  speaker: Kristina Monakhova
  speaker_image: AdithyaPediredla.jpeg
  institution: PhD Candidate, UC Berkeley
  description:
  summary: "Some animals, such as hawkmoths and carpenter bees, can effectively navigate on the darkest moonless nights by the light of the stars (< 0.001 lux), whereas our best CMOS cameras generally require at least 3/4 moon illumination (> 0.1 lux) to image moving objects at night. Videography on moonless, clear nights is extremely challenging due to low photon counts. Long exposures can be used, but precludes the imaging of moving objects. Alternatively, higher gain can make pixels more sensitive to light, but leads to increased noise which is often non-Gaussian, sensor-specific, and difficult to model or characterize. Off-the-shelf denoisers which assume Gaussian noise often fail in these low light, high gain regimes due to this structured noise. In this work, we present a learned, physics-informed noise model which, after training, can more accurately synthesize camera noise at the lowest light and highest gain settings. Using our noise model, we train a video denoiser using a combination of synthetic noisy video clips and real still images. We capture a 5-10 fps video dataset with significant motion on a moonless, clean night with no active illumination (0.6-0.7 mililux). Using our video denoiser, we demonstrate photorealistic video denoising in starlight (< 0.001 lux) for the first time."
  date: 6
  month_year: May, 2022
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Computational imaging with multiply scattered photons"
  speaker: Adithya Pediredla
  speaker_image: AdithyaPediredla.jpeg
  institution: Project Scientist, Carnegie Mellon University
  description:
  summary: "Computational imaging has advanced to a point where the next significant milestone is to image in the presence of multiply-scattered light. Though traditionally treated as noise, multiply-scattered light carries information that can enable previously impossible imaging capabilities, such as imaging around corners and deep inside tissue. The combinatorial complexity of multiply-scattered light transport makes it necessary to use increasingly complex imaging systems to make imaging with multiply-scattered light feasible; examples include time-of-flight, structured-light, and acousto-optic imaging systems. The combined complexity of physics and systems makes the optimization of imaging of multiply-scattered light a challenging, high-dimensional design problem. In my research, I utilize graphics and physics-based rendering to explore this complex design space and create imaging systems that optimally sense multiply-scattered light. I will show two examples of this approach. First, I will discuss how to develop rendering tools for time-of-flight cameras, and how to use these tools to design and build optimal time-of-flight systems for non-line-of-sight imaging. Second, I will discuss how to simulate continuously-refractive radiative transfer and use such simulations to optimize acousto-optic systems for imaging inside tissue and other scattering media."
  date: 1
  month_year: April, 2022
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Utilizing Nature’s Limits for better Computational 3D Imaging"
  speaker: Florian Willomitzer
  speaker_image: FWillomitzer_compressed_squared.jpg
  institution: Research Assistant Professor, Northwestern University
  description:
  summary: "Computational 3D imaging and 3D display principles are ‘enabling technologies’ that have the potential to foster transformational technical changes in the next decades. The list of possible future applications scenarios is long: Novel breeds of cameras could see through tissue, fog, or around corners. Portable medical 3D imaging and 3D display devices could effectively assist doctors in diagnosis and therapy. Precise and fast 3D scanners could become essential for measuring and analyzing dynamic scenes during robotic surgery, industrial inspection, autonomous navigation, or cultural heritage applications. Novel 3D display and eye-tracking methods could finally enable immersive AR/VR experiences. These are just a few examples. In midst of these seemingly endless possibilities, the knowledge about fundamental physical and information-theoretical limits in imaging proves to be a powerful tool: By knowing that our imaging device already operates at the physical limit (e.g., of resolution), we can avoid unnecessary effort and investments in better hardware such as faster detectors, or cameras with higher pixel resolution. Moreover, limits often appear as uncertainty products, making it possible to optimize our system towards a specific quantity (e.g., speed) by trading in information less critical for the respective application. Although the imaging device is essential in this optimization, the central role is assumed by the illumination, which serves as encoder of the desired information. In this talk, I will discuss the virtue of limits and merit of illumination modalities in computational 3D imaging systems using the examples of my research discussed above. Among other systems, I will introduce a novel method to image hidden objects through scattering media and around corners, and a ‘single-shot 3D camera’ for the motion-robust, precise, and dense 3D measurement of fast live scenes."
  date: 4
  month_year: March, 2022
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Foundational Geometric Vision and its Role in Modern 3D Data-Acquisition Methods"
  speaker: Suryansh Kumar
  speaker_image: suryansh_kumar.JPG
  institution: Senior Researcher, CVL group at ETH Zurich
  description:
  summary: "In the coming decade, dense, detailed, and precise 3D data acquisition of objects or scenes is sure to become one of the most important problems in computer vision and industrial machine vision. Moreover, it can be helpful for a wide range of other cutting-edge scientific disciplines such as metrology, geometry processing, forensics, etc. Unfortunately, at present, we don't have a general-purpose vision algorithm or framework for dense 3D data acquisition to meet the required precision. In this talk, I will present a few of our recent ideas and algorithms showing how a mindful use of existing classical geometric ideas can help us get closer to the goal of high-fidelity 3D data acquisition from images or available depth-sensing modalities. Furthermore, the talk will briefly discuss the dense 3D data acquisition methods using an online robot system and its current challenges. To conclude, this talk will pivot around the blueprint of classical geometric vision ideas and how to use it to effectively exploit the strength of deep neural networks for more trustworthy 3D data acquisition."
  date: 4
  month_year: February, 2022
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Artificial Intelligence in Dermatology: Opportunities and Challenges"
  speaker: Roxana Daneshjou
  speaker_image: Roxana_Daneshjou.jpg
  institution: Postdoc, Stanford School of Medicine
  description:
  summary: "The clinical practice of dermatology is visual in nature: physicians make decisions after either directly assessing the skin or viewing clinical photos.  Deep learning algorithms have the potential to streamline processes in dermatology and teledermatology. However, the development of generalizable and robust clinical algorithms will require diverse, high quality datasets. I will discuss our work in developing a deep learning algorithm for assessing photo quality in teledermatology. After completing initial in silico testing, we are translating this work from “bytes” to “bedside”.  Additionally, I will discuss the challenges for developing fair and robust algorithms in dermatology and what steps we can take to address these challenges."
  date: 5
  month_year: November, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Computational astronomical imaging: inference and sensing algorithms for future black hole observations"
  speaker: He Sun
  speaker_image: he_sun.jpeg
  institution: Postdoc, Caltech
  description:
  summary: "Recent development in computational imaging has led to a series of great discoveries in astronomy, such as the first image of a black hole (M87*) acquired by the Event Horizon Telescope (EHT). In this talk, I will introduce two new computational imaging algorithms we have developed for advancing future black hole observations. First, I will introduce Deep Probabilistic Imaging: a novel variational inference algorithm that trains a deep generative model to quantify the uncertainty of a reconstructed image. This approach helps to better characterize the confidence of structures in a black hole reconstruction, leading to more reliable scientific interpretations. Second, I will present “cosense”, a physics-informed deep learning approach that jointly optimizes a telescope network design with an image reconstruction approach. This algorithm is currently being used to select new telescope locations for the next generation Event Horizon Telescope (ngEHT). I will also briefly show the general applications of these algorithms in other scientific imaging tasks, such as exoplanet discovery and MRI."
  date: 1
  month_year: October, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "One Photon at a Time: Compensating for Nonideal Electronics in LiDAR Imaging"
  speaker: Joshua Rapp
  speaker_image: joshua_rapp.jpg
  institution: Research Scientist at Mitsubishi Electric Research Laboratories
  description: 
  summary: "Forming a digital image—with a conventional camera or computational imaging system—requires converting properties of light into a sequence of bits. The mechanisms transforming optical energy into digital signals can often be idealized or ignored, but problems such as quantization or saturation become noticeable when imaging at the limits of the sensors. One such example arises in the case of single-photon lidar, which aims to form 3D images from individual photon detections. In this talk, we consider two factors that prevent perfect recording of each photon arrival time with infinite precision: finite temporal quantization and missed detections during detector dead times. We show that incorporating nonidealities into our acquisition models significantly mitigate their effect on our ability to form accurate depth images."
  date: 2
  month_year: July, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Enabling an Image-Based Graphics Pipeline with Neural Radiance Fields"
  speaker: Pratul Srinivasan
  speaker_image: pratul_srinivasan.jpg
  institution: Google Research
  description: 
  summary: Neural Radiance Fields (NeRFs) have recently emerged as an effective and simple solution for recovering 3D representations of complex objects and scenes from captured images. However, there is still a long way to go before we will be able to use NeRF-like representations of real-world content in graphics pipelines as easily as standard computer graphics representations of artist-designed content, such as textured triangle meshes. I will discuss and review NeRF, and then talk about some of our recent work towards extending NeRF to enable more of the functionality we expect from the 3D representations we use in computer graphics.
  date: 4
  month_year: June, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: "Digital Health and Wellbeing: Data-Driven and Human-Centered Personalized and Adaptive Assistant"
  speaker: Professor Akane Sano
  speaker_image: AkaneSano_003 - trimmed.JPG
  institution: Assistant Professor at Rice University
  description: 
  summary: Imagine 24/7 rich human multimodal data could identify changes in physiology and behavior, and provide personalized early warnings to help you, patients, or clinicians for making better decisions or behavioral changes to support health and wellbeing. I will introduce a series of studies, algorithms, and systems we have developed for measuring, predicting, and supporting personalized health and wellbeing for clinical populations as well as people at increased risk of adverse events, including ongoing COVID-19 related projects. I will also discuss challenges, learned lessons, and potential future directions in digital health and wellbeing research.
  date: 7
  month_year: May, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: Differential Defocus in Cameras and Microscopes
  speaker: Dr. Emma Alexander
  speaker_image: EA.jpg
  institution: Postdoc, UC Berkeley
  description: 
  summary: Image defocus provides a useful depth cue in computer vision, and can also be used to recover phase information in coherent microscopy. In a differential setting, both problems can be addressed by solving simple equations, known as Depth from Differential Defocus and the Transport of Intensity Equation. Relating these governing equations requires putting them on equal footing, so we'll look at the assumptions common to photography and microscopy applications, and go through a gentle introduction to coherence, light fields and Wigner Distribution Functions, and generalized phase. We'll show that depth from defocus can be seen as a special case of phase recovery, with a new interpretation of phase for incoherent settings.
  date: 2
  month_year: April, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
- title: Can cameras really measure vital signs? Algorithms and systems for camera-based health monitoring in unconstrained settings.
  speaker: Ewa Nowara
  speaker_image: Ewa_Nowara_Picture.png
  institution: PhD at Rice University
  description: 
  summary: Imagine when you looked at someone, you could see their heartbeat. A suite of techniques called imaging photoplethysmography has recently enabled contactless measurements of vital signs with cameras by leveraging small intensity changes in the skin caused by cardiac activity. Measuring vital signs remotely is advantageous in several applications, including virtual doctor appointments, especially relevant during a pandemic, as well as more comfortable sleep monitoring, or monitoring of prematurely born infants. However, the camera-based physiological signals are very weak and easily corrupted by varying illumination, video compression artifacts, and head motion. Therefore, most existing methods only work in controlled settings and fail in realistic applications. We developed a denoising deep learning algorithm based on convolutional attention networks that can faithfully recover physiological signals even from heavily corrupted videos. Moreover, our denoising algorithm can recover subtle waveform dynamics, previously not possible to measure with cameras. We also discuss how we can improve the performance of deep learning methods and avoid overfitting when training on very small and not diverse datasets.
  date: 5
  month_year: March, 2021
  time: 12 noon PT
  link: https://docs.google.com/forms/d/e/1FAIpQLSdAo7SiHvafHPPOp-FnfoW9fRWHEBzYybTAaP3d_Z4FdCnt0A/viewform
